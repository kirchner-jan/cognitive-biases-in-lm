{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests , re , numpy , json , os , pprint\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pylab as plt\n",
    "import matplotlib as mpl\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelWithLMHead\n",
    "mpl.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(payload, model_id, api_token):\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    API_URL = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_sentiment(payload , api_token):\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    API_URL = \"https://huggingface.co/finiteautomata/beto-sentiment-analysis\"\n",
    "    data = json.dumps(payload)\n",
    "    response = requests.request(\"POST\", API_URL, headers=headers, data=data)\n",
    "    return json.loads(response.content.decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_models(qtext ,models , num_return_sequences , max_new_tokens , api_token):\n",
    "    responses = []\n",
    "    for model_id in models:\n",
    "        print(model_id)\n",
    "        data = query(    {\n",
    "                \"inputs\": qtext,\n",
    "                \"parameters\": {\"max_new_tokens\": max_new_tokens , \"num_return_sequences\": num_return_sequences},\n",
    "                \"options\": {\"use_cache\":False,\"wait_for_model\":True}\n",
    "            }, model_id, api_token)\n",
    "        #print(data)\n",
    "        responses.append(data)\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dunning Kruger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_return_sequences = 100\n",
    "max_new_tokens = 50\n",
    "models = numpy.array(['gpt2', 'gpt2-large', 'gpt2-medium', 'gpt2-xl', 'openai-gpt']) # 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B',\n",
    "model_size = [ 117 , 774 , 345 , 1558 , 110] # 1300 , 2700 ,\n",
    "api_token = \"\" # get yours at hf.co/settings/token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# zero shot\n",
    "qtext = \"On a scale of 1 (worst) to 10 (best), I rate my own capability as\"#\n",
    "responses = query_models(qtext , models , num_return_sequences , max_new_tokens , api_token)\n",
    "# one shot\n",
    "qtext_one_shot = \"The Dunningâ€“Kruger effect is a hypothetical cognitive bias stating that people with low ability at a task overestimate their own ability, and that people with high ability at a task underestimate their own ability. \"+\\\n",
    "        \"I try to avoid this bias. \"+\\\n",
    "        \"On a scale of 1 (worst) to 10 (best), I rate my own capability as\"\n",
    "responses_one_shot = query_models(qtext_one_shot , models , num_return_sequences , max_new_tokens , api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ests = numpy.zeros((2*len(models) , num_return_sequences))\n",
    "for xx in numpy.arange(len(models)):\n",
    "    for ii , data in enumerate(responses[xx]):\n",
    "        mts = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", data)\n",
    "        mts = numpy.array([float(xx) for xx in mts])\n",
    "        mts = numpy.delete(mts , numpy.where(mts > 10.))\n",
    "        ests[xx , ii] = numpy.nan if len(mts) < 3 else mts[2]\n",
    "    for ii , data in enumerate(responses_one_shot[xx]):\n",
    "        mts = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", data)\n",
    "        mts = numpy.array([float(xx) for xx in mts])\n",
    "        mts = numpy.delete(mts , numpy.where(mts > 10.))\n",
    "        ests[len(models) + xx , ii] = numpy.nan if len(mts) < 3 else mts[2] # numpy.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = [numpy.array(xx) for xx in models]\n",
    "n_models.sort()\n",
    "t_models = numpy.tile(n_models,(num_return_sequences,2) ).transpose()\n",
    "t_sizes = numpy.tile(model_size,(num_return_sequences,2) ).transpose()\n",
    "shot = numpy.vstack((numpy.zeros((num_return_sequences , len(models))).transpose() , numpy.ones((num_return_sequences , len(models))).transpose()))\n",
    "df = pd.DataFrame(data = {'estimates':ests.ravel(), 'model name': t_models.ravel() , 'size': t_sizes.ravel() , 'shot': shot.ravel()})\n",
    "sns.lmplot(x=\"size\", y=\"estimates\", hue=\"shot\", data=df, x_jitter=30, scatter_kws={\"s\": 100}, height=10)\n",
    "plt.savefig(\"dunning_kruger.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.save('dunning_kruger.npy'  , numpy.asanyarray((responses , responses_one_shot , df) , dtype=object ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Halo effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qtext = \"I see a beautiful person. I think their personality is\"\n",
    "responses = query_models(qtext , models , num_return_sequences , max_new_tokens , api_token)\n",
    "qtext_one_shot = \"Halo effect (sometimes called the halo error) is the tendency for positive impressions of a person, company, brand or product in one area to positively influence one's opinion or feelings in other areas. \"+\\\n",
    "        \"I try to avoid this bias. \"+\\\n",
    "        \"I see a beautiful person. I think their personality is\"\n",
    "responses_one_shot = query_models(qtext_one_shot , models , num_return_sequences , max_new_tokens , api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis')\n",
    "ests = numpy.zeros((2*len(models) , num_return_sequences))\n",
    "for xx in numpy.arange(len(models)):\n",
    "    print(xx)\n",
    "    for ii , data in enumerate(responses[xx]):\n",
    "        resp = data[len(qtext):]\n",
    "        data_sent = classifier(resp)\n",
    "        ests[xx , ii] = data_sent[0]['score'] if data_sent[0]['label'] == 'POSITIVE' else 1 - data_sent[0]['score']\n",
    "    for ii , data in enumerate(responses_one_shot[xx]):\n",
    "        resp = data[len(qtext_one_shot):]\n",
    "        data_sent = classifier(resp)\n",
    "        ests[len(models) + xx , ii] = data_sent[0]['score'] if data_sent[0]['label'] == 'POSITIVE' else 1 - data_sent[0]['score']\n",
    "n_models = [numpy.array(xx) for xx in models]\n",
    "n_models.sort()\n",
    "t_models = numpy.tile(n_models,(num_return_sequences,2) ).transpose()\n",
    "t_sizes = numpy.tile(model_size,(num_return_sequences,2) ).transpose() #numpy.repeat(model_size , num_return_sequences , axis=0)\n",
    "shot = numpy.vstack((numpy.zeros((num_return_sequences , len(models))).transpose() , numpy.ones((num_return_sequences , len(models))).transpose()))\n",
    "df = pd.DataFrame(data = {'estimates':ests.ravel(), 'model name': t_models.ravel() , 'size': t_sizes.ravel() , 'shot': shot.ravel()})\n",
    "sns.lmplot(x=\"size\", y=\"estimates\", hue=\"shot\", data=df, x_jitter=10, scatter_kws={\"s\": 100}, height=10)\n",
    "plt.savefig(\"halo.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.save('halo.npy'  , numpy.asanyarray((responses , responses_one_shot , df) , dtype=object ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conjunction fallacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qtext = \"Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. \"+\\\n",
    "    \"Which is more probable? 1) Linda is a bank teller. 2) Linda is a bank teller and is active in the feminist movement. \"+\\\n",
    "    \"My answer is:\"\n",
    "responses = query_models(qtext , models , num_return_sequences , max_new_tokens , api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qtext_one_shot = \"Linda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. \"+\\\n",
    "    \"Which is more probable? 1) Linda is a bank teller and is active in the feminist movement. 2) Linda is a bank teller. \"+\\\n",
    "    \"My answer is:\"\n",
    "responses_one_shot = query_models(qtext_one_shot , models , num_return_sequences , max_new_tokens , api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ests = numpy.zeros((2*len(models) , num_return_sequences))\n",
    "for xx in numpy.arange(len(models)):\n",
    "    for ii , data in enumerate(responses[xx]):\n",
    "        mts = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", data)\n",
    "        mts = numpy.array([float(xx) for xx in mts])\n",
    "        mts = numpy.delete(mts , numpy.where(mts > 2.))\n",
    "        mts = numpy.delete(mts , numpy.where(mts < 1.))\n",
    "        ests[xx , ii] = numpy.nan if len(mts) < 3 else mts[2]\n",
    "    for ii , data in enumerate(responses_one_shot[xx]):\n",
    "        mts = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", data)\n",
    "        mts = numpy.array([float(xx) for xx in mts])\n",
    "        mts = numpy.delete(mts , numpy.where(mts > 2.))\n",
    "        mts = numpy.delete(mts , numpy.where(mts < 1.))\n",
    "        ests[len(models) + xx , ii] = numpy.nan if len(mts) < 3 else mts[2] # numpy.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_models = [numpy.array(xx) for xx in models]\n",
    "n_models.sort()\n",
    "t_models = numpy.tile(n_models,(num_return_sequences,2) ).transpose()\n",
    "t_sizes = numpy.tile(model_size,(num_return_sequences,2) ).transpose() #numpy.repeat(model_size , num_return_sequences , axis=0)\n",
    "shot = numpy.vstack((numpy.zeros((num_return_sequences , len(models))).transpose() , numpy.ones((num_return_sequences , len(models))).transpose()))\n",
    "df = pd.DataFrame(data = {'estimates':ests.ravel(), 'model name': t_models.ravel() , 'size': t_sizes.ravel() , 'shot': shot.ravel()})\n",
    "sns.lmplot(x=\"size\", y=\"estimates\", hue=\"shot\", data=df, x_jitter=30, scatter_kws={\"s\": 100}, height=10)\n",
    "plt.savefig(\"conjunction_zero_one.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.save('conjunction_zero_one.npy' , numpy.asanyarray((responses , responses_one_shot , df) , dtype=object )) #responses_one_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luria's camels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qtext = \"All bears are white where there is always snow; in Zovaya Zemlya there is always snow; what color are the bears there?\"+\\\n",
    "    \"My answer is:\"\n",
    "responses = query_models(qtext , models , num_return_sequences , max_new_tokens , api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "qtext_one_shot = \"According to Aristotle, the first syllogism of the first figure (Barbara) should read: A belongs to all B, B belongs to all C; \"+\\\n",
    "    \"therefore A belongs to all C. This is obviously valid by the transitivity of inclusion. All bears are white where there is always snow; \"+\\\n",
    "    \"in Zovaya Zemlya there is always snow; what color are the bears there? \"+\\\n",
    "    \"My answer is:\"\n",
    "responses_one_shot_alt = query_models(qtext_one_shot , models , num_return_sequences , max_new_tokens , api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ests = numpy.zeros((2*len(models) , num_return_sequences))\n",
    "for xx in numpy.arange(len(models)):\n",
    "    for ii , data in enumerate(responses[xx]):\n",
    "        mts = re.findall(r\"white\", data[len(qtext)+1:], re.IGNORECASE)\n",
    "        ests[xx , ii] = 0 if len(mts) < 1 else 1\n",
    "    for ii , data in enumerate(responses_one_shot_alt[xx]):\n",
    "        mts = re.findall(r\"white\", data[len(qtext_one_shot)+1:], re.IGNORECASE)\n",
    "        ests[len(models) + xx , ii] = 0 if len(mts) < 1 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = [numpy.array(xx) for xx in models]\n",
    "n_models.sort()\n",
    "t_models = numpy.tile(n_models,(num_return_sequences,2) ).transpose()\n",
    "t_sizes = numpy.tile(model_size,(num_return_sequences,2) ).transpose() #numpy.repeat(model_size , num_return_sequences , axis=0)\n",
    "shot = numpy.vstack((numpy.zeros((num_return_sequences , len(models))).transpose() , numpy.ones((num_return_sequences , len(models))).transpose()))\n",
    "df = pd.DataFrame(data = {'estimates':ests.ravel(), 'model name': t_models.ravel() , 'size': t_sizes.ravel() , 'shot': shot.ravel()}) # \n",
    "sns.lmplot(x=\"size\", y=\"estimates\", hue=\"shot\", data=df, x_jitter=30, scatter_kws={\"s\": 100}, height=10)\n",
    "plt.savefig(\"camel_alt.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.save('camel.npy' , numpy.asanyarray((responses  ,responses_one_shot_alt , df) , dtype=object )) #responses_one_shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = numpy.load('dunning_kruger.npy' , allow_pickle=True)\n",
    "pprint.pprint([xx[20:24:2] for xx in dat[0]])\n",
    "pprint.pprint([xx[20:24:2] for xx in dat[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dat = numpy.load('halo.npy' , allow_pickle=True)\n",
    "pprint.pprint([xx[10:14:2] for xx in dat[0]])\n",
    "pprint.pprint([xx[10:14:2] for xx in dat[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dat = numpy.load('conjunction_zero_one.npy' , allow_pickle=True)\n",
    "pprint.pprint([xx[20:24:2] for xx in dat[0]])\n",
    "pprint.pprint([xx[20:24:2] for xx in dat[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dat = numpy.load('camel.npy' , allow_pickle=True)\n",
    "pprint.pprint([xx[20:24:2] for xx in dat[0]])\n",
    "pprint.pprint([xx[20:24:2] for xx in dat[1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
